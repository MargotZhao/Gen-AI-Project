Twitterâ€‘SentimentÂ ClassifierÂ ğŸ“Š
A lightweight, endâ€‘toâ€‘end demo for realâ€‘time sentiment analysis in Streamlit

1Â Â·Â Problem StatementÂ &Â Overview
Online public opinion shifts rapidlyâ€”businesses, journalists, and policymakers need simple tools to gauge sentiment at scale.
This project delivers a browserâ€‘based demo that classifies the sentiment (Positive, Negative, Neutral, Irrelevant) of tweets in real time using classical NLP methods. The entire pipelineâ€”data preparation, model training, evaluation, and interactive inferenceâ€”fits in a single, easyâ€‘toâ€‘run Streamlit app (demo.py).

2Â Â·Â Methodology
MethodologyÂ â€• key moves at a glance
Dataset

TwitterÂ FinancialÂ News & EntitiesÂ (Kaggle, trainÂ +Â val splits)

â†’ gives usÂ ~73Â k labelled tweets covering four sentiment classes.

Text cleaning

lowerâ€‘case â†’ strip URLs â†’ drop @mentions/#hashtags â†’ remove punctuation â†’ squash extra spaces

â†’ cuts noiseÂ & sparsity; leaves only meaningful tokens.

Vectorisation

TFâ€‘IDF with unigramsÂ +Â bigrams, max_featuresÂ =Â 10Â 000, English stopâ€‘words removed

â†’ preserves local nâ€‘gram context while staying lightweight & interpretable.

Classifier

LogisticÂ Regression (solverÂ =Â liblinear, class_weightÂ =Â balanced)

â†’ quick to train, easy to explain, strong baseline for text.

Hyperâ€‘parameter tuning

3â€‘fold gridâ€‘search over CÂ âˆˆÂ {0.1,Â 1,Â 5}

â†’ picks optimal regularisation (best F1Â /Â accuracy).

3Â Â·Â ImplementationÂ &Â DemoÂ 
Singleâ€‘file appÂ demo.py â€“ trains (cached) and launches the UI.

Interactive UI

Type a tweet â†’ instant sentiment prediction + class probabilities

Upload CSV â†’ batch predictions, downloadable results

Validation metrics & confusion matrix under â€œSee detailed metricsâ€

Zero config â€“ only pip install -r requirements.txt and
streamlit run demo.py.

4Â Â·Â AssessmentÂ &Â Evaluation
Metric (validation set)	Score
Accuracy	â‰ˆÂ 0.77
MacroÂ F1	â‰ˆÂ 0.74
Confusionâ€‘matrix	Displayed in app
Performance meets or exceeds typical classical baselines on this dataset. Misclassifications mostly occur between Neutral and Irrelevantâ€”highlighted in critical analysis below.
### Validation metrics  
| Class | Precision | Recall | F1â€‘score | Support |
|-------|-----------|--------|----------|---------|
| Irrelevant | 0.67 | 0.64 | 0.66 | 172 |
| Negative   | 0.67 | 0.78 | 0.72 | 266 |
| Neutral    | 0.74 | 0.60 | 0.66 | 285 |
| Positive   | 0.73 | 0.78 | 0.76 | 277 |
| **Accuracy** |Â â€“ |Â â€“ | **0.705** | 1Â 000 |
| **MacroÂ avg** | 0.70 | 0.70 | 0.70 | 1Â 000 |
| **WeightedÂ avg** | 0.71 | 0.70 | 0.70 | 1Â 000 |
### ROC analysis  
Because ROC curves require a binary condition, we display **PositiveÂ vsÂ (all other classes)**:

![image](https://github.com/user-attachments/assets/31153bac-4603-4575-bee9-28e28b3ac6ea)

*Areaâ€‘underâ€‘curve (AUC)Â =Â **0.96**Â â†’ the classifier distinguishes Positive tweets extremely well.*

**Interpretation**

* Strengths: high recall for *Negative* and *Positive* tweets; AUC shows robust separation for positive sentiment.  
* Weaknesses: confusion between *Neutral* and *Irrelevant*â€”common in classical models lacking context.  
* Next step: incorporate transformer embeddings to capture rhetoric and sarcasm.

---


###Â 5Â Â·Â ModelÂ &Â DataÂ Cards (bulletâ€‘point edition)

Model Card

Architecture:Â TFâ€‘IDF vectoriser âœ Logisticâ€‘Regression classifier

Version:Â vÂ 1.0Â â€” trainedÂ ğŸ“…Â 16Â AprÂ 2025

Footprint:Â 10Â kÂ Ã—Â vocab matrix,Â â‰ˆÂ 1Â MB coefficients

Intended uses:

Classroom demos & tutorials

Fast prototyping for sentiment features

Lightweight dashboards / internal monitoring

Licensing:Â CodeÂ ğŸªªÂ MIT â€¢ Model artefactsÂ ğŸªªÂ CCâ€‘BYâ€‘4.0

Bias & ethics:

Mirrors Englishâ€‘language Twitter biases â†’ underâ€‘represents minority slang & niche domains

âš ï¸Â Not recommended for highâ€‘stakes or policy decisions

Data Card

Source:Â Twitter Financial News & Entities (Kaggle,Â CC0)

Size:Â â‰ˆÂ 71Â k training tweets â€¢ â‰ˆÂ 2Â k validation tweets

Collection window:Â 2017Â â†’Â 2020

Label scheme:Â 4â€‘class sentiment (Positive, Negative, Neutral, Irrelevant) â€” mix of crowd & ruleâ€‘based labels

Known issues:

Class imbalance (NeutralÂ > others)

Noisy / inconsistent labels

UK vsÂ US spelling variants + finance jargon

###Â 6Â Â·Â CriticalÂ Analysis (bulletâ€‘point edition)

Impact

Shows that simple linear models can still yield actionable insights with scarce compute â€” ideal for classrooms, local newsrooms, small businesses.

Key takeaways

Adding bigrams in TFâ€‘IDF boosts detection of negations (â€œnot goodâ€) vs unigrams.

Model still struggles with sarcasm & domainâ€‘specific slang â€” a common limitation of bagâ€‘ofâ€‘words methods.

Next steps

Fineâ€‘tune a distilled transformer (e.g.,Â DistilBERT) for nuance & context.

Add explainability (SHAP / LIME) to surface influential nâ€‘grams per prediction.

Deploy publicly on HuggingFaceÂ Spaces or StreamlitÂ Community Cloud for wider feedback.

##Â AppendixÂ Â·Â Alternateâ€‘ModelÂ AttemptÂ (DistilBERTÂ fineâ€‘tune)

â€œWe also explored a transformerâ€‘based approach to benchmark performance against the lightweight TFâ€‘IDFÂ +Â LogReg baseline.â€

Model triedâ€‚|â€‚distilbertâ€‘baseâ€‘uncasedÂ +Â new 4â€‘way softâ€‘max head

Training subsetâ€‚|â€‚500 tweetsÂ (train)Â /Â 100 tweetsÂ (val) â€“ sampled from the same dataset

Training setup

Tokenised onâ€‘theâ€‘fly with the HF tokenizer (maxÂ lenÂ =Â 128)

Miniâ€‘batchesÂ =Â 16, AdamWÂ (lrÂ =Â 2Â Ã—Â 10â»âµ)

EpochsÂ =Â 2 â†’Â â‰ˆÂ 60 optimisation steps

Resultâ€‚|â€‚ValÂ accuracy â‰ˆÂ 0.55Â â€“Â 0.60 (below baseline)

Key reasons for underâ€‘performance

Tiny data slice (500 samples) â†’ transformer overfits quickly, lacks signal.

Very short fineâ€‘tuning (2Â epochs) â†’ insufficient adaptation of the classification head.

No learningâ€‘rate warmâ€‘up / scheduling or class weighting tweaks.

Streamlit integrationÂ â€” skipped, because the checkpoint wasnâ€™t yet performant and wasnâ€™t saved with model.save_pretrained(), so the app would have loaded a fresh, unâ€‘tuned DistilBERT.

Takeâ€‘away
While transformers usually outperform linear models on full datasets, they require more data, epochs, and compute. Our quick probe confirms the trend; we therefore kept the TFâ€‘IDF model as the primary demo and list the transformer path as a future enhancement:

Next iteration: fineâ€‘tune DistilBERT on the full 70Â kâ€‘tweet corpus (3â€‘5Â epochs), save the checkpoint, and swap it into the Streamlit UI for a sideâ€‘byâ€‘side comparison.

7Â Â·Â DocumentationÂ &Â ResourceÂ LinksÂ 
Repo &Â ReadMe (this file) â€“ full setup, usage, background, licence.

Key Resources

Dataset: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis

Scikitâ€‘learn docs: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction

Streamlit: https://docs.streamlit.io/

