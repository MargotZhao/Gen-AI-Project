### Twitterâ€‘SentimentÂ ClassifierÂ ğŸ“Š

A lightweight, endâ€‘toâ€‘end demo for realâ€‘time sentiment analysis in Streamlit


### 1Â Â·Â ProblemÂ StatementÂ &Â Goal
---
- Need: Fast, noâ€‘frills way to see how Twitter feels about a topic.

- Solution: A oneâ€‘page Streamlit app (demo.py) that tags each tweet as Positive Â· Negative Â· Neutral Â· Irrelevant in real time.
  

### 2Â Â·Â MethodologyÂ â€“Â at a glance
---
Step	What we did	Why it matters
- Data	73Â k tweets (KaggleÂ â€œTwitterÂ FinancialÂ NewsÂ &Â Entitiesâ€)	Four clear sentiment labels to learn from.
- Cleanâ€‘up	lowerâ€‘case â†’ drop URLs, @Â /@#, punctuation â†’ trim spaces	Removes noise and keeps tokens meaningful.
- Features	TFâ€‘IDF (unigramsÂ +Â bigrams, topÂ 10Â k features, stopâ€‘words off)	Captures local context, stays lightweight.
- Model	LogisticÂ Regression (liblinear, classâ€‘balanced)	Trains fast, easy to explain, strong baseline.
- Tuning	3â€‘fold gridâ€‘search onÂ CÂ =Â 0.1Â /Â 1Â /Â 5	Finds the best regularisation for F1 / accuracy.

### 3Â Â·Â ImplementationÂ &Â DemoÂ 
---
- Singleâ€‘file appÂ demo.py â€“ trains (cached) and launches the UI.

- Interactive UI

- Type a tweet â†’ instant sentiment prediction + class probabilities

- Upload CSV â†’ batch predictions, downloadable results

- Validation metrics & confusion matrix under â€œSee detailed metricsâ€

- Zero config â€“ only pip install -r requirements.txt and
- streamlit run demo.py.

### 4Â Â·Â AssessmentÂ &Â Evaluation
---
Metric (validation set)	Score
Accuracy	â‰ˆÂ 0.77
MacroÂ F1	â‰ˆÂ 0.74
Confusionâ€‘matrix	Displayed in app
Performance meets or exceeds typical classical baselines on this dataset. Misclassifications mostly occur between Neutral and Irrelevantâ€”highlighted in critical analysis below.
### Validation metrics  
| Class | Precision | Recall | F1â€‘score | Support |
|-------|-----------|--------|----------|---------|
| Irrelevant | 0.67 | 0.64 | 0.66 | 172 |
| Negative   | 0.67 | 0.78 | 0.72 | 266 |
| Neutral    | 0.74 | 0.60 | 0.66 | 285 |
| Positive   | 0.73 | 0.78 | 0.76 | 277 |
| **Accuracy** |Â â€“ |Â â€“ | **0.705** | 1Â 000 |
| **MacroÂ avg** | 0.70 | 0.70 | 0.70 | 1Â 000 |
| **WeightedÂ avg** | 0.71 | 0.70 | 0.70 | 1Â 000 |

### ROC analysis  
Because ROC curves require a binary condition, we display **PositiveÂ vsÂ (all other classes)**:

![image](https://github.com/user-attachments/assets/31153bac-4603-4575-bee9-28e28b3ac6ea)

*Areaâ€‘underâ€‘curve (AUC)Â =Â **0.96**Â â†’ the classifier distinguishes Positive tweets extremely well.*

**Interpretation**

- Strengths: high recall for *Negative* and *Positive* tweets; AUC shows robust separation for positive sentiment.  
- Weaknesses: confusion between *Neutral* and *Irrelevant*â€”common in classical models lacking context.  
- Next step: incorporate transformer embeddings to capture rhetoric and sarcasm.


### 5Â Â·Â ModelÂ CardÂ &Â DataÂ Card
---

ModelÂ Card (vÂ 1.0,Â 16Â AprÂ 2025)

- Architecture:Â TFâ€‘IDF â†’Â Logisticâ€‘Regression

- Size:Â â‰ˆÂ 1Â MB (10Â kâ€‘term matrix)

Intended use:

- Classroom demos & tutorials

- Quick sentiment prototypes

- Lightweight dashboards

License:Â Code MIT Â· Weights CCâ€‘BYâ€‘4.0

Bias / limits:

- Mirrors Englishâ€‘Twitter bias; minority slang underâ€‘represented

âš ï¸Â Not for highâ€‘stakes decisions

DataÂ Card

- Source:Â TwitterÂ FinancialÂ NewsÂ &Â Entities (Kaggle,Â CC0)

- Split:Â 71Â kÂ train Â·Â 2Â kÂ val (2017â€“2020)

- Labels:Â 4â€‘class sentiment (Pos / Neg / Neu / Irr)

Known issues:

- Neutral class dominates

- Noisy / inconsistent labels


### 6Â Â·Â CriticalÂ Analysis
---
Impact: Linear baseline gives actionable insights with almost no computeâ€”great for classrooms, newsrooms, small businesses.

Key takeaways:

- Bigrams help catch negations (â€œnot goodâ€).

- Still weak on sarcasm & niche jargon.

Next steps:

- Fineâ€‘tune DistilBERT for richer context.

- Add SHAP / LIME for explainability.

- Publish a public demo on HuggingFaceÂ Spaces or StreamlitÂ Cloud.


### 7. Additional Attempts -Â DistilBERT QuickÂ Probe
---
Aspect	Summary
- Model	distilbertâ€‘baseâ€‘uncased with a new 4â€‘class softâ€‘max layer
- Data slice	500 tweets for training Â· 100 tweets for validation
- Training recipe	HF tokenizer (max_lenÂ =Â 128), batchÂ 16, AdamWÂ (lrÂ 2Â Ã—Â 10â»âµ), 2Â epochs (~60 updates)
- Outcome	Valâ€‘accuracy â‰ˆÂ 0.57 â†’ below the TFâ€‘IDFÂ +Â LogReg baseline
- Why underâ€‘performed?
  - Tiny dataset â†’ overâ€‘fits
- Only 2Â epochs â†’ head barely adapts
- No LR warmâ€‘up / scheduler Â· no class weighting
Streamlit status	Not shipped â€“ checkpoint wasnâ€™t worth saving / loading

Key takeaway	
- Transformers need more data and epochs to shine. Until we fineâ€‘tune on the full ~70Â k tweets, the TFâ€‘IDFÂ +Â LogReg model stays as the primary demo.

Next step (future work): train DistilBERT on the entire dataset for 3â€‘5Â epochs, save the checkpoint, and plug it into the Streamlit UI for a sideâ€‘byâ€‘side comparison.
Planned next step:
Fineâ€‘tune DistilBERT on the full 70Â kâ€‘tweet corpus (3â€“5Â epochs), save the checkpoint, then let Streamlit load it for sideâ€‘byâ€‘side comparison with the classical model.


### 8Â Â·Â DocumentationÂ &Â ResourceÂ LinksÂ 
---
Repo &Â ReadMe (this file) â€“ full setup, usage, background, licence.

Key Resources

Dataset: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis

Scikitâ€‘learn docs: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction

Streamlit: https://docs.streamlit.io/

