Twitterâ€‘SentimentÂ ClassifierÂ ğŸ“Š
A lightweight, endâ€‘toâ€‘end demo for realâ€‘time sentiment analysis in Streamlit

1Â Â·Â Problem StatementÂ &Â Overview
Online public opinion shifts rapidlyâ€”businesses, journalists, and policymakers need simple tools to gauge sentiment at scale.
This project delivers a browserâ€‘based demo that classifies the sentiment (Positive, Negative, Neutral, Irrelevant) of tweets in real time using classical NLP methods. The entire pipelineâ€”data preparation, model training, evaluation, and interactive inferenceâ€”fits in a single, easyâ€‘toâ€‘run Streamlit app (demo.py).

2Â Â·Â Methodology
Step	Technique	Why it matters
Dataset	Twitter Financial News & Entities (trainingÂ +Â validation splits from Kaggle)	Provides labelled tweets with diverse sentiment.
Text cleaning	Lowerâ€‘casing, URL/user/hashtag removal, punctuation stripping, whitespace normalization	Reduces noise and sparsity.
Vectorisation	TFâ€‘IDF (unigramsÂ +Â bigrams, max_features=10Â 000, English stopâ€‘words)	Captures local nâ€‘gram context while staying lightweight.
Classifier	Logistic Regression (liblinear solver, classâ€‘balanced loss)	Fast, interpretable baseline suitable for small demos.
Hyperâ€‘parameter search	Gridâ€‘search (C âˆŠ {0.1,Â 1,Â 5}) with 3â€‘fold CV	Selects regularisation strength for best F1/accuracy.
Techniques draw directly on course topics: text preâ€‘processing, sparse vector space models, linear classifiers, crossâ€‘validation.

3Â Â·Â ImplementationÂ &Â DemoÂ 
Singleâ€‘file appÂ demo.py â€“ trains (cached) and launches the UI.

Interactive UI

Type a tweet â†’ instant sentiment prediction + class probabilities

Upload CSV â†’ batch predictions, downloadable results

Validation metrics & confusion matrix under â€œSee detailed metricsâ€

Zero config â€“ only pip install -r requirements.txt and
streamlit run demo.py.

4Â Â·Â AssessmentÂ &Â Evaluation
Metric (validation set)	Score
Accuracy	â‰ˆÂ 0.77
MacroÂ F1	â‰ˆÂ 0.74
Confusionâ€‘matrix	Displayed in app
Performance meets or exceeds typical classical baselines on this dataset. Misclassifications mostly occur between Neutral and Irrelevantâ€”highlighted in critical analysis below.
![image](https://github.com/user-attachments/assets/77783726-4a5b-4c40-9f9f-a8c5c3f15f9d)
### ROC analysis  
Because ROC curves require a binary condition, we display **PositiveÂ vsÂ (all other classes)**:

![ROC curve](assets/roc_positive_vs_others.png)

*Areaâ€‘underâ€‘curve (AUC)Â =Â **0.96**Â â†’ the classifier distinguishes Positive tweets extremely well.*

**Interpretation**

* Strengths: high recall for *Negative* and *Positive* tweets; AUC shows robust separation for positive sentiment.  
* Weaknesses: confusion between *Neutral* and *Irrelevant*â€”common in classical models lacking context.  
* Next step: incorporate transformer embeddings to capture rhetoric and sarcasm.

---


5Â Â·Â ModelÂ &Â DataÂ CardsÂ 
Model Card
Field	Details
Model	TFâ€‘IDF Vectoriser â†’ Logistic Regression
Version	1.0 (trained 2025â€‘04â€‘16)
Size	10Â 000Â Ã—Â vocab matrix, ~1Â MB coefficients
Intended Uses	Classroom demos, prototyping, sentiment dashboards
Licensing	Code MIT, model artefacts CCâ€‘BYâ€‘4.0
Bias & Ethics	Reflects biases in Englishâ€‘language Twitterâ€”may underâ€‘represent minorities, slang, or nonâ€‘financial topics. Not suitable for highâ€‘stakes decisions.
Data Card
Field	Details
Source	Twitter Financial News & Entities (Kaggle, CC0)
Size	TrainingÂ â‰ˆÂ 71Â k tweets, ValidationÂ â‰ˆÂ 2Â k tweets
Collection Period	2017â€‘2020
Label Scheme	4â€‘class sentiment, crowdsourced & heuristic labels
Known Issues	Class imbalance (NeutralÂ >Â others), noisy labels, UK/US spelling variance
6Â Â·Â CriticalÂ AnalysisÂ 
Impact â€“ Demonstrates how even simple linear models deliver actionable insights in resourceâ€‘constrained settings (e.g., journalism classrooms, small businesses).

Reveals â€“ Bigram TFâ€‘IDF improves detection of negations (â€œnot goodâ€) over plain unigrams; yet still mistakes sarcasm & domainâ€‘specific jargon.

Next Steps

Fineâ€‘tune a distilled transformer (DistilBERT) for better nuance.

Add explainability (e.g., SHAP for feature importance).

Deploy on HuggingFace Spaces for public access.

7Â Â·Â DocumentationÂ &Â ResourceÂ LinksÂ 
Repo &Â ReadMe (this file) â€“ full setup, usage, background, licence.

Key Resources

Dataset: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis

Scikitâ€‘learn docs: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction

Streamlit: https://docs.streamlit.io/

